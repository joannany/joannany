### Hi, Iâ€™m Anna

I'm a Senior AI Product Manager working on how machine learning systems are evaluated and monitored after deployment.

I've built and led projects around:
- **Post-deployment model monitoring** (real-world drift detection, calibration analysis, and performance evaluation in medical AI systems)
- **Statistical evaluation frameworks** (approaches that go beyond single scores, using null distributions, threshold sensitivity, and comparative analysis to reason about model behavior)
- **Model lineage and independence analysis** (methods for assessing whether models likely share training or fine-tuning history, based on weight and representation level evidence rather than raw similarity claims)
- **LLM behavior stability** (ways to detect silent behavior changes and consistency issues in deployed language model) â€” in progress

My work sits close to research but is driven by practical questions teams face in production, especailly in settings where errors have real consequences.

---

### Featured Projects
- **Model Lineage Audit**  
  Statistical toolkit for analyzing whether two models likely share training lineage. Uses null-relative similarity.
  
- **Model Eval & Drift Lab**  
  Tools for detecting distribution shift, assessing calibration, and stress-testing deployed ML systems.

- **GPT-Drift**  
  Lightweight behavioral fingerprinting to detect silent behavior changes in LLM APIs.
  
---

### ðŸ“« Connect
<a href="https://linkedin.com/in/anna-jo-72088a21" target="_blank">
  <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white"/>
</a>
